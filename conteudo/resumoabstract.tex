%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

% As palavras-chave são obrigatórias, em português e em inglês, e devem ser
% definidas antes do resumo/abstract. Acrescente quantas forem necessárias.
\palavraschave{IA, Aprendizado de Máquina, IA Explicável, XAI, Visualização de Características, GradCam, LIME}

\keywords{AI, Machine Learning, Explainable AI, XAI, Feature visualization, GradCam, LIME}

% O resumo é obrigatório, em português e inglês. Estes comandos também
% geram automaticamente a referência para o próprio documento, conforme
% as normas sugeridas da USP.
\resumo{
Com a ascensão do uso de Aprendizado de Máquina para problemas de Visão Computacional, o uso de Redes Neurais Convolucionais (CNNs) se mostrou uma peça fundamental para a criação de modelos estado da arte em tarefas como classificação, detecção de objetos e até mesmo segmentação.
No entanto,em muitos casos, a simples obtenção do resultado de uma predição não é suficiente, sendo necessária uma justificativa para as decisões do modelo.
Utilizando IA Explicável (XAI), podemos encontrar possíveis explicações para predições de modelos complexos como Redes Convolucionais. 
Nesse trabalho, foram estudadas diversas técnicas de Explicabilidade aplicadas a CNNs, utilizando de técnicas como GradCam e Visualização de Características. 
Além disso, foram conduzidos experimentos com cada técnica abordada visando avaliar a eficácia na interpretação dos modelos de Visão Computacional.
}

\abstract{
With the rise of Machine Learning in Computer Vision problems, the use of Convolutional Neural Networks (CNNs) has proven to be a fundamental component in developing state-of-the-art models for tasks such as classification, object detection, and even segmentation.
However, in many cases, simply obtaining the result of a prediction is not sufficient; a justification for the model's decisions is necessary.
By employing Explainable AI (XAI), it is possible to identify potential explanations for the predictions of complex models such as Convolutional Neural Networks.
In this study, various explainability techniques applied to CNNs were analyzed, utilizing methods such as Grad-CAM and Feature Visualization.
Additionally, experiments were conducted with each technique to assess their effectiveness in interpreting Computer Vision models.
}
